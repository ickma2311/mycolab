{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO+isxlQpFq756oik0uyktj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ickma2311/mycolab/blob/main/LLM_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Explanation\n"
      ],
      "metadata": {
        "id": "By37-4Ft-vKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CN](https://photos.google.com/photo/AF1QipMyeNzqcwt2s9b-tpfxHpky1dTBSf7yfm1GOtGn)\n",
        "\n",
        "PPO’s Clipped Objective\n",
        "\n",
        "The Proximal Policy Optimization (PPO) “clipped” loss is defined as:\n",
        "\n",
        "$$\n",
        "{L} = \\mathbb{E}\\Bigl[\\min\\bigl(\\rho(\\theta)\\,A,\\;\\mathrm{clip}(\\rho(\\theta),\\,1-\\epsilon,\\,1+\\epsilon)\\,A\\bigr)\\Bigr].\n",
        "$$\n",
        "\n",
        "### Components\n",
        "*\tProbability ratio\n",
        "\n",
        "$\\rho(\\theta) = \\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_{\\rm old}}(a\\mid s)}$\n",
        "\n",
        "measures how much the new policy \\pi_\\theta differs from the old policy $\\pi_{\\theta_{\\rm old}} $ in state s taking action a.\n",
        "\n",
        "\n",
        "* Advantage A\n",
        "\n",
        "$A = r + \\gamma\\,V(s’) - V(s)$\n",
        "\n",
        "(or another estimator) reflects how much better taking action a in state s is compared to the baseline.\n",
        "\n",
        "* Clipping\n",
        "\n",
        "  * Unclipped term: $ \\rho(\\theta)\\,A $    \n",
        "  * Clipped term: $\\mathrm{clip}(\\rho(\\theta),\\,1-\\epsilon,\\,1+\\epsilon)\\,A$.    \n",
        "  * We take the minimum of these two so that when $\\rho$ moves outside $[1-\\epsilon,1+\\epsilon]$, the update is limited.    \n",
        "\n",
        "* Expectation\n",
        "$\\mathbb{E}[\\cdot]$ indicates averaging over all sampled state–action pairs (e.g., a batch).\n",
        "\n",
        "### Intuition\n",
        "1.\tNo change.   \n",
        "When $\\rho(\\theta)\\approx1$, the loss reduces to the standard policy-gradient term $\\rho A$.\n",
        "2.\tPreventing large updates\n",
        "If $\\rho$ deviates from 1 by more than \\epsilon, the clipped term caps it at $1\\pm\\epsilon$.    \n",
        "\t•\tFor $A>0$, taking the clipped term prevents overly large increases.    \n",
        "\t•\tFor $A<0$, taking the unclipped term prevents overly large decreases.     \n",
        "3.\tStable, efficient learning\n",
        "This design keeps the benefits of importance sampling (low variance) while enforcing a trust-region–like constraint, all without requiring second-order methods.\n"
      ],
      "metadata": {
        "id": "M9MfDcapr3DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval original Model"
      ],
      "metadata": {
        "id": "2gf4Fesu-S5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt\n"
      ],
      "metadata": {
        "id": "3_vqzZPX-_cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 4096 # Can increase for longer reasoning traces\n",
        "lora_rank = 64 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.3, # Reduce if out of memory\n",
        ")\n"
      ],
      "metadata": {
        "id": "UHCaLmuE_yv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n"
      ],
      "metadata": {
        "id": "FqLokpbHGwrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "!pip install fsspec==2023.9.2\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "ds = load_dataset(\"TIGER-Lab/MMLU-Pro\")"
      ],
      "metadata": {
        "id": "eypNYZ3iItdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds=ds['test'].select(range(1000))"
      ],
      "metadata": {
        "id": "VDzHWa_e4B5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import ascii_uppercase\n",
        "ascii_uppercase"
      ],
      "metadata": {
        "id": "Gwn78WoZyhbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\n",
        "\"\"\"\n",
        "from tqdm import tqdm\n",
        "from string import ascii_uppercase\n",
        "\n",
        "text=[]\n",
        "for item in tqdm(test_ds):\n",
        "    t=f\"The following are single choice questions  about {item['category']}. Answer the choice index, e.g. A/B/C ....\"\n",
        "    t+=SYSTEM_PROMPT\n",
        "    t+=f'\\nQuestion: {item[\"question\"]}'\n",
        "    choices=item['options']\n",
        "    for index,choice in zip(ascii_uppercase[:len(choices)],choices):\n",
        "        t+=f'\\n{index}) {choice}'\n",
        "    t+='\\n Answer:'\n",
        "\n",
        "    text.append(t)\n",
        "\n",
        "text_with_template=[tokenizer.apply_chat_template([\n",
        "        # {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "        {\"role\":\"user\",\n",
        "         'content':t}\n",
        "    ],tokenize=False,add_generation_prompt=True)\n",
        "\n",
        "      for t in text]\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.1,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "outputs=model.fast_generate(\n",
        "    text_with_template,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        "    use_tqdm=True\n",
        "\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TLrLNSbmGznn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_answer(output_):\n",
        "  try:\n",
        "    return output_.split('<answer>')[-1].split('</answer>')[0].strip()\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "\n",
        "def correctness(output_):\n",
        "  correct=0\n",
        "  for o,a in zip(output_,test_ds['answer']):\n",
        "    answer=parse_answer(o.outputs[0].text)\n",
        "    if answer and answer[0]==a:\n",
        "      correct+=1\n",
        "  return correct/len(output_)\n",
        "\n"
      ],
      "metadata": {
        "id": "9VB_xvMNpXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctness(outputs)"
      ],
      "metadata": {
        "id": "PKnEQXQjwqum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Train"
      ],
      "metadata": {
        "id": "_OomJbYN7vG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "2Y1VaT9k72pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def correctness_reward(completions,answer,**kwargs):\n",
        "  return [5 if a==parse_answer(c) else 0 for (c,a) in zip(completions,answer)]\n",
        "\n",
        "def len_reward(completions,**kwargs):\n",
        "  return [len(c)//1000 if len(c)<3000 else 0 for c in completions]\n",
        "\n",
        "def format_reward_cal(c,a):\n",
        "  reward=0\n",
        "  if c.find('<reasoning>')!=-1:\n",
        "    reward+=0.25\n",
        "  if c.find('</reasoning>')!=-1:\n",
        "    reward+=0.25\n",
        "  if c.find('<answer>')!=-1:\n",
        "    reward+=0.25\n",
        "  if c.find('</answer>')!=-1:\n",
        "    reward+=0.25\n",
        "  if len(c.split('<answer>')[-1].split('</answer>')[0].strip())==len(a):\n",
        "    reward+=1\n",
        "  return reward\n",
        "\n",
        "def format_reward(completions,answer,**kwargs):\n",
        "  return [format_reward_cal(c,a) for c,a in zip(completions,answer)]\n",
        "\n",
        "def exrtact_answer(a):\n",
        "  try:\n",
        "    return a.split('####')[1].strip()\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "train_dataset_=load_dataset('openai/gsm8k','main')\n",
        "train_dataset=[]\n",
        "for item in train_dataset_['train']:\n",
        "  prompt=tokenizer.apply_chat_template([\n",
        "        {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "        {\"role\":\"user\",\n",
        "         'content':item['question']}],\n",
        "                                       tokenize=False,\n",
        "                                      add_generation_prompt=True)\n",
        "  answer=exrtact_answer(item['answer'])\n",
        "  train_dataset.append({'prompt':prompt,'answer':answer})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MyPEeqeq78yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-5,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 64,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 8, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 1024,\n",
        "    num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 200,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ],
      "metadata": {
        "id": "8jDpB4oDN6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "       correctness_reward,\n",
        "       format_reward\n",
        "      #  len_reward\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EJKDOZ2KN9oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "tm0GSjX2EVPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=model.fast_generate(\n",
        "    text_with_template,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        "    use_tqdm=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "QXtplkiImq5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctness(outputs)"
      ],
      "metadata": {
        "id": "wmzymB7Omwd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}