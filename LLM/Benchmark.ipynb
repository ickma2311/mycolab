{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyORpESVGcZMwto8SLvqudMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88565711e2064f368951906c07d7c6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_88d71b6140024fa6a53ab4a4be8dc83f"
          }
        },
        "0d5fbb59ae3b4744bf21baaef3812e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99993ba4b59940a48241831325df5551",
            "placeholder": "​",
            "style": "IPY_MODEL_ac71b300e246491a8a8df6e2a6e6fe63",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "bab2ce2a0913422f9f409e2e6891055c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9baf3af162fe40849dbea3edbb0269f0",
            "placeholder": "​",
            "style": "IPY_MODEL_36dff7321acf4ef9bd9e9b23a1fd4c4b",
            "value": ""
          }
        },
        "93a6b63f6d654ba5985a0fc87b81d6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7840aaeb5f9a44debc78f93ad13a6956",
            "style": "IPY_MODEL_7065945243ef4c6392a020df47f33841",
            "value": true
          }
        },
        "6f42456292bb436a85df28638ba36339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b96c8d7b60fa47fe8b63a12a23fff8eb",
            "style": "IPY_MODEL_3a9103b4edcb49069f0366d9288877a4",
            "tooltip": ""
          }
        },
        "faaa07b417c4464a898bf537c77bad89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_350852edeaf64840b60e97e8d2889075",
            "placeholder": "​",
            "style": "IPY_MODEL_0228a0879c2c4625bad7c54fda4565fe",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "88d71b6140024fa6a53ab4a4be8dc83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "99993ba4b59940a48241831325df5551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac71b300e246491a8a8df6e2a6e6fe63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9baf3af162fe40849dbea3edbb0269f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36dff7321acf4ef9bd9e9b23a1fd4c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7840aaeb5f9a44debc78f93ad13a6956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7065945243ef4c6392a020df47f33841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b96c8d7b60fa47fe8b63a12a23fff8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9103b4edcb49069f0366d9288877a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "350852edeaf64840b60e97e8d2889075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0228a0879c2c4625bad7c54fda4565fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f13177ebcb994a8cb3dfca4163555a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ffff5163f51468eaa7dda06eda381f0",
            "placeholder": "​",
            "style": "IPY_MODEL_12947b7af95245eea281dcdac6c6843e",
            "value": "Connecting..."
          }
        },
        "9ffff5163f51468eaa7dda06eda381f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12947b7af95245eea281dcdac6c6843e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ickma2311/mycolab/blob/main/LLM/Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness && pip install -e \".[math,ifeval,sentencepiece]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAIBMadr4-1Y",
        "outputId": "08343581-4f2f-4d74-839b-c755ca8f645e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 10358, done.\u001b[K\n",
            "remote: Counting objects: 100% (10358/10358), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4334/4334), done.\u001b[K\n",
            "remote: Total 10358 (delta 6053), reused 9836 (delta 5994), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10358/10358), 3.51 MiB | 26.03 MiB/s, done.\n",
            "Resolving deltas: 100% (6053/6053), done.\n",
            "Obtaining file:///content/lm-evaluation-harness\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (1.6.0)\n",
            "Collecting evaluate (from lm_eval==0.4.8)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.16.0 (from lm_eval==0.4.8)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting jsonlines (from lm_eval==0.4.8)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (0.15.2)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.8)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.8)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.8)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.8)\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval==0.4.8)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (2.6.0+cu124)\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.8)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (4.51.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (0.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (0.3.7)\n",
            "Collecting word2number (from lm_eval==0.4.8)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (10.7.0)\n",
            "Collecting langdetect (from lm_eval==0.4.8)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (4.2.1)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (3.9.1)\n",
            "Requirement already satisfied: sympy>=1.12 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (1.13.1)\n",
            "Collecting antlr4-python3-runtime==4.11 (from lm_eval==0.4.8)\n",
            "  Downloading antlr4_python3_runtime-4.11.0-py3-none-any.whl.metadata (291 bytes)\n",
            "Collecting math_verify[antlr4_11_0] (from lm_eval==0.4.8)\n",
            "  Downloading math_verify-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.1.98 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.8) (0.2.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.31.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.8) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->lm_eval==0.4.8) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->lm_eval==0.4.8) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->lm_eval==0.4.8) (2024.11.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8) (1.17.0)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.8)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8) (5.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.12->lm_eval==0.4.8) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.8)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.8) (3.2.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm_eval==0.4.8) (0.21.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->lm_eval==0.4.8) (25.3.0)\n",
            "Collecting latex2sympy2_extended==1.10.1 (from math_verify[antlr4_11_0]; extra == \"math\"->lm_eval==0.4.8)\n",
            "  Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval==0.4.8) (75.2.0)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0->lm_eval==0.4.8) (1.1.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.8) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.8) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.8) (3.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.8) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8) (1.20.0)\n",
            "Downloading antlr4_python3_runtime-4.11.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading math_verify-0.7.0-py3-none-any.whl (28 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: lm_eval, rouge-score, langdetect, sqlitedict, word2number\n",
            "  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm_eval: filename=lm_eval-0.4.8-0.editable-py3-none-any.whl size=25018 sha256=b8486ba04e67ea978fb09573075cc60e8c5c92c0ad75b1a51d82de5e029a9c8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4nc04mlx/wheels/84/1c/11/502a8926c958091ff989c1ae74d66aade33728f4ab83f77d87\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f68965b3865b7afadc50317eeace87ffd9045a0fadc4df0200bebdb333a96387\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=1d4d2599a2122e9107e1f9e67d47bc4104ae5ba7576d6bcc511c9d1d9e91c7bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=c5754edcf2011ae74ce5698c82f98e458ad6de0bd0deb17ec362fba2ed816bba\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=8d8fa787dfa9edcd280375f5518ac815de5497607388881fa5bef9075d2b56c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built lm_eval rouge-score langdetect sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, antlr4-python3-runtime, tcolorpy, pybind11, portalocker, pathvalidate, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mbstrdecoder, langdetect, jsonlines, fsspec, colorama, typepy, tqdm-multiprocess, sacrebleu, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, latex2sympy2_extended, nvidia-cusolver-cu12, math_verify, datasets, DataProperty, tabledata, evaluate, pytablewriter, lm_eval\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.1.0 antlr4-python3-runtime-4.11.0 colorama-0.4.6 datasets-3.6.0 evaluate-0.4.3 fsspec-2025.3.0 jsonlines-4.0.0 langdetect-1.0.9 latex2sympy2_extended-1.10.1 lm_eval-0.4.8 math_verify-0.7.0 mbstrdecoder-1.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathvalidate-3.2.3 portalocker-3.1.1 pybind11-2.13.6 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GVpykmrmUr71"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install lm_eval[vllm]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "88565711e2064f368951906c07d7c6e4",
            "0d5fbb59ae3b4744bf21baaef3812e09",
            "bab2ce2a0913422f9f409e2e6891055c",
            "93a6b63f6d654ba5985a0fc87b81d6eb",
            "6f42456292bb436a85df28638ba36339",
            "faaa07b417c4464a898bf537c77bad89",
            "88d71b6140024fa6a53ab4a4be8dc83f",
            "99993ba4b59940a48241831325df5551",
            "ac71b300e246491a8a8df6e2a6e6fe63",
            "9baf3af162fe40849dbea3edbb0269f0",
            "36dff7321acf4ef9bd9e9b23a1fd4c4b",
            "7840aaeb5f9a44debc78f93ad13a6956",
            "7065945243ef4c6392a020df47f33841",
            "b96c8d7b60fa47fe8b63a12a23fff8eb",
            "3a9103b4edcb49069f0366d9288877a4",
            "350852edeaf64840b60e97e8d2889075",
            "0228a0879c2c4625bad7c54fda4565fe",
            "f13177ebcb994a8cb3dfca4163555a57",
            "9ffff5163f51468eaa7dda06eda381f0",
            "12947b7af95245eea281dcdac6c6843e"
          ]
        },
        "id": "LzccAN08AmGQ",
        "outputId": "455fa4a8-117d-4f20-9c7b-e6e3025f270a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88565711e2064f368951906c07d7c6e4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=Qwen/Qwen3-4B,dtype=auto\\\n",
        "    --tasks leaderboard \\\n",
        "    --batch_size auto \\\n",
        "    --output_path results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJGsdesE0aqE",
        "outputId": "e5461290-4d51-4de9-a183-50409b72b6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-15 16:41:23.513274: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-15 16:41:23.530256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747327283.551758  138713 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747327283.558252  138713 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-15 16:41:23.579836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 05-15 16:41:28 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-15 16:41:28 [__init__.py:239] Automatically detected platform cuda.\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['leaderboard']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'Qwen/Qwen3-4B', 'dtype': 'auto'}\n",
            "INFO:lm_eval.models.huggingface:Using device 'cuda'\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "Loading checkpoint shards: 100% 3/3 [00:02<00:00,  1.20it/s]\n",
            "INFO:lm_eval.evaluator:leaderboard_math_algebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_counting_and_prob_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_geometry_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_intermediate_algebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_num_theory_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_prealgebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_precalculus_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_mmlu_pro on rank 0...\n",
            "100% 12032/12032 [00:00<00:00, 12333.16it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_boolean_expressions on rank 0...\n",
            "100% 250/250 [00:00<00:00, 337.32it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_causal_judgement on rank 0...\n",
            "100% 187/187 [00:00<00:00, 337.93it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_date_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 338.67it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_disambiguation_qa on rank 0...\n",
            "100% 250/250 [00:00<00:00, 341.12it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_formal_fallacies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 338.54it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_geometric_shapes on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.52it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_hyperbaton on rank 0...\n",
            "100% 250/250 [00:00<00:00, 345.03it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 342.05it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 341.96it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 341.90it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_movie_recommendation on rank 0...\n",
            "100% 250/250 [00:00<00:00, 344.59it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_navigate on rank 0...\n",
            "100% 250/250 [00:00<00:00, 344.51it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_object_counting on rank 0...\n",
            "100% 250/250 [00:00<00:00, 333.93it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_penguins_in_a_table on rank 0...\n",
            "100% 146/146 [00:00<00:00, 337.27it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_reasoning_about_colored_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 338.41it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_ruin_names on rank 0...\n",
            "100% 250/250 [00:00<00:00, 344.61it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_salient_translation_error_detection on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.51it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_snarks on rank 0...\n",
            "100% 178/178 [00:00<00:00, 339.54it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_sports_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 347.08it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_temporal_sequences on rank 0...\n",
            "100% 250/250 [00:00<00:00, 342.65it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 341.41it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 343.69it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 344.57it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_web_of_lies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 345.11it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_diamond on rank 0...\n",
            "100% 198/198 [00:00<00:00, 1259.07it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_extended on rank 0...\n",
            "100% 546/546 [00:00<00:00, 1256.97it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_main on rank 0...\n",
            "100% 448/448 [00:00<00:00, 1261.91it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_algebra_hard on rank 0...\n",
            "100% 307/307 [00:00<00:00, 363.91it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_counting_and_prob_hard on rank 0...\n",
            "100% 123/123 [00:00<00:00, 343.30it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_geometry_hard on rank 0...\n",
            "100% 132/132 [00:00<00:00, 366.27it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_intermediate_algebra_hard on rank 0...\n",
            "100% 280/280 [00:00<00:00, 367.55it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_num_theory_hard on rank 0...\n",
            "100% 154/154 [00:00<00:00, 361.70it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_prealgebra_hard on rank 0...\n",
            "100% 193/193 [00:00<00:00, 364.18it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_precalculus_hard on rank 0...\n",
            "100% 135/135 [00:00<00:00, 365.69it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_ifeval on rank 0...\n",
            "100% 541/541 [00:00<00:00, 105349.29it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_murder_mysteries on rank 0...\n",
            "100% 250/250 [00:00<00:00, 2386.87it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_object_placements on rank 0...\n",
            "100% 256/256 [00:00<00:00, 2367.15it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_team_allocation on rank 0...\n",
            "100% 250/250 [00:00<00:00, 2401.27it/s]\n",
            "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
            "Running loglikelihood requests:   0% 0/152668 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
            "Determined largest batch size: 1\n",
            "Running loglikelihood requests:   9% 13681/152668 [10:34<2:37:00, 14.75it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !lm_eval --model vllm \\\n",
        "#     --model_args pretrained=Qwen/Qwen3-4B,dtype=auto,gpu_memory_utilization=0.8,max_model_len=40000\\\n",
        "#     --tasks leaderboard \\\n",
        "#     --batch_size 1 \\\n",
        "#     --output_path results\n",
        "\n",
        "# vllm raise OOM when running loglikehood"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tnBhAs36K5q",
        "outputId": "1a8215e3-ba65-460d-8db0-6f8780e58831"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-15 16:24:24.130442: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-15 16:24:24.148171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747326264.169779  134136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747326264.176290  134136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-15 16:24:24.197758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 05-15 16:24:28 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-15 16:24:28 [__init__.py:239] Automatically detected platform cuda.\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['leaderboard']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing vllm model, with arguments: {'pretrained': 'Qwen/Qwen3-4B', 'dtype': 'auto', 'gpu_memory_utilization': 0.8, 'max_model_len': 40000}\n",
            "INFO 05-15 16:24:50 [config.py:717] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 05-15 16:24:50 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 05-15 16:24:52 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
            "2025-05-15 16:24:56.733196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747326296.754003  134360 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747326296.760319  134360 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO 05-15 16:25:01 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-15 16:25:01 [__init__.py:239] Automatically detected platform cuda.\n",
            "INFO 05-15 16:25:04 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-4B', speculative_config=None, tokenizer='Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=Qwen/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 05-15 16:25:04 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a3e51891d90>\n",
            "INFO 05-15 16:25:05 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-15 16:25:05 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "WARNING 05-15 16:25:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 05-15 16:25:05 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-4B...\n",
            "INFO 05-15 16:25:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO 05-15 16:25:08 [loader.py:458] Loading weights took 2.60 seconds\n",
            "INFO 05-15 16:25:09 [gpu_model_runner.py:1347] Model loading took 7.5552 GiB and 3.863418 seconds\n",
            "INFO 05-15 16:25:23 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/06b597e41c/rank_0_0 for vLLM's torch.compile\n",
            "INFO 05-15 16:25:23 [backends.py:430] Dynamo bytecode transform time: 14.67 s\n",
            "INFO 05-15 16:25:35 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 10.018 s\n",
            "INFO 05-15 16:25:37 [monitor.py:33] torch.compile takes 14.67 s in total\n",
            "INFO 05-15 16:25:39 [kv_cache_utils.py:634] GPU KV cache size: 61,920 tokens\n",
            "INFO 05-15 16:25:39 [kv_cache_utils.py:637] Maximum concurrency for 40,000 tokens per request: 1.55x\n",
            "INFO 05-15 16:26:15 [gpu_model_runner.py:1686] Graph capturing finished in 36 secs, took 0.57 GiB\n",
            "INFO 05-15 16:26:15 [core.py:159] init engine (profile, create kv cache, warmup model) took 66.29 seconds\n",
            "INFO 05-15 16:26:15 [core_client.py:439] Core engine process 0 ready.\n",
            "README.md: 100% 963/963 [00:00<00:00, 7.17MB/s]\n",
            "murder_mystery.csv: 100% 1.40M/1.40M [00:00<00:00, 1.57MB/s]\n",
            "object_placements.csv: 100% 1.30M/1.30M [00:00<00:00, 5.76MB/s]\n",
            "team_allocation.csv: 100% 871k/871k [00:00<00:00, 2.04MB/s]\n",
            "Generating murder_mysteries split: 100% 250/250 [00:00<00:00, 9671.16 examples/s]\n",
            "Generating object_placements split: 100% 256/256 [00:00<00:00, 14106.65 examples/s]\n",
            "Generating team_allocation split: 100% 250/250 [00:00<00:00, 16030.58 examples/s]\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "Downloaded punkt_tab on rank 0\n",
            "README.md: 100% 28.0/28.0 [00:00<00:00, 195kB/s]\n",
            "input_data.jsonl: 100% 207k/207k [00:00<00:00, 964kB/s]\n",
            "Generating train split: 100% 541/541 [00:00<00:00, 49183.25 examples/s]\n",
            "README.md: 100% 8.41k/8.41k [00:00<00:00, 35.6MB/s]\n",
            "train-00000-of-00001.parquet: 100% 354k/354k [00:00<00:00, 30.1MB/s]\n",
            "test-00000-of-00001.parquet: 100% 242k/242k [00:00<00:00, 58.2MB/s]\n",
            "Generating train split: 100% 746/746 [00:00<00:00, 22898.39 examples/s]\n",
            "Generating test split: 100% 546/546 [00:00<00:00, 125401.93 examples/s]\n",
            "Filter: 100% 546/546 [00:00<00:00, 87857.36 examples/s]\n",
            "Map: 100% 135/135 [00:00<00:00, 7370.11 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 384k/384k [00:00<00:00, 56.8MB/s]\n",
            "test-00000-of-00001.parquet: 100% 268k/268k [00:00<00:00, 70.9MB/s]\n",
            "Generating train split: 100% 1205/1205 [00:00<00:00, 195359.14 examples/s]\n",
            "Generating test split: 100% 871/871 [00:00<00:00, 225633.92 examples/s]\n",
            "Filter: 100% 871/871 [00:00<00:00, 117331.67 examples/s]\n",
            "Map: 100% 193/193 [00:00<00:00, 8409.70 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 309k/309k [00:00<00:00, 69.4MB/s]\n",
            "test-00000-of-00001.parquet: 100% 182k/182k [00:00<00:00, 99.9MB/s]\n",
            "Generating train split: 100% 869/869 [00:00<00:00, 179903.76 examples/s]\n",
            "Generating test split: 100% 540/540 [00:00<00:00, 160382.68 examples/s]\n",
            "Filter: 100% 540/540 [00:00<00:00, 99000.09 examples/s]\n",
            "Map: 100% 154/154 [00:00<00:00, 8048.18 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 575k/575k [00:00<00:00, 84.7MB/s]\n",
            "test-00000-of-00001.parquet: 100% 395k/395k [00:00<00:00, 103MB/s]\n",
            "Generating train split: 100% 1295/1295 [00:00<00:00, 161992.95 examples/s]\n",
            "Generating test split: 100% 903/903 [00:00<00:00, 190784.63 examples/s]\n",
            "Filter: 100% 903/903 [00:00<00:00, 120091.84 examples/s]\n",
            "Map: 100% 280/280 [00:00<00:00, 8433.30 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 549k/549k [00:00<00:00, 114MB/s]\n",
            "test-00000-of-00001.parquet: 100% 264k/264k [00:00<00:00, 122MB/s]\n",
            "Generating train split: 100% 870/870 [00:00<00:00, 137887.11 examples/s]\n",
            "Generating test split: 100% 479/479 [00:00<00:00, 136365.41 examples/s]\n",
            "Filter: 100% 479/479 [00:00<00:00, 92197.31 examples/s]\n",
            "Map: 100% 132/132 [00:00<00:00, 7704.32 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 329k/329k [00:00<00:00, 122MB/s]\n",
            "test-00000-of-00001.parquet: 100% 175k/175k [00:00<00:00, 159MB/s]\n",
            "Generating train split: 100% 771/771 [00:00<00:00, 184947.58 examples/s]\n",
            "Generating test split: 100% 474/474 [00:00<00:00, 161581.61 examples/s]\n",
            "Filter: 100% 474/474 [00:00<00:00, 92962.69 examples/s]\n",
            "Map: 100% 123/123 [00:00<00:00, 7699.07 examples/s]\n",
            "train-00000-of-00001.parquet: 100% 505k/505k [00:00<00:00, 128MB/s]\n",
            "test-00000-of-00001.parquet: 100% 353k/353k [00:00<00:00, 140MB/s]\n",
            "Generating train split: 100% 1744/1744 [00:00<00:00, 264897.02 examples/s]\n",
            "Generating test split: 100% 1187/1187 [00:00<00:00, 254219.71 examples/s]\n",
            "Filter: 100% 1187/1187 [00:00<00:00, 129628.42 examples/s]\n",
            "Map: 100% 307/307 [00:00<00:00, 8825.82 examples/s]\n",
            "README.md: 100% 9.43k/9.43k [00:00<00:00, 42.6MB/s]\n",
            "test-00000-of-00001.parquet: 100% 15.6k/15.6k [00:00<00:00, 96.5MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 110121.40 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 25.1k/25.1k [00:00<00:00, 138MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 101576.67 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 49.1k/49.1k [00:00<00:00, 187MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 94313.37 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 37.1k/37.1k [00:00<00:00, 133MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 86033.48 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 35.6k/35.6k [00:00<00:00, 134MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 87967.79 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 8.16k/8.16k [00:00<00:00, 56.3MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 115215.47 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 16.4k/16.4k [00:00<00:00, 88.8MB/s]\n",
            "Generating test split: 100% 178/178 [00:00<00:00, 80624.85 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 56.9k/56.9k [00:00<00:00, 222MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 81958.42 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 15.5k/15.5k [00:00<00:00, 83.9MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 102120.76 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 20.4k/20.4k [00:00<00:00, 117MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 82708.31 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 10.7k/10.7k [00:00<00:00, 68.4MB/s]\n",
            "Generating test split: 100% 146/146 [00:00<00:00, 63914.87 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 10.6k/10.6k [00:00<00:00, 68.6MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 106986.63 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 10.0k/10.0k [00:00<00:00, 60.1MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 104941.55 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 21.7k/21.7k [00:00<00:00, 102MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 102330.05 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 21.6k/21.6k [00:00<00:00, 73.3MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 89028.36 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 44.0k/44.0k [00:00<00:00, 170MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 81385.90 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 33.5k/33.5k [00:00<00:00, 133MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 94051.13 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 10.4k/10.4k [00:00<00:00, 68.3MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 104408.64 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 20.2k/20.2k [00:00<00:00, 89.5MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 102460.04 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 35.8k/35.8k [00:00<00:00, 166MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 97988.60 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 16.7k/16.7k [00:00<00:00, 77.7MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 106013.14 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 18.0k/18.0k [00:00<00:00, 85.9MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 106801.39 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 69.5k/69.5k [00:00<00:00, 212MB/s]\n",
            "Generating test split: 100% 187/187 [00:00<00:00, 61710.06 examples/s]\n",
            "test-00000-of-00001.parquet: 100% 4.70k/4.70k [00:00<00:00, 35.3MB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 113703.75 examples/s]\n",
            "README.md: 100% 10.9k/10.9k [00:00<00:00, 55.1MB/s]\n",
            "test-00000-of-00001.parquet: 100% 4.16M/4.16M [00:00<00:00, 168MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 45.3k/45.3k [00:00<00:00, 187MB/s]\n",
            "Generating test split: 100% 12032/12032 [00:00<00:00, 384270.54 examples/s]\n",
            "Generating validation split: 100% 70/70 [00:00<00:00, 24491.26 examples/s]\n",
            "INFO:lm_eval.evaluator:leaderboard_math_algebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_counting_and_prob_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_geometry_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_intermediate_algebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_num_theory_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_prealgebra_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_math_precalculus_hard: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1024}\n",
            "INFO:lm_eval.evaluator:leaderboard_ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_mmlu_pro on rank 0...\n",
            "100% 12032/12032 [00:00<00:00, 12294.83it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_boolean_expressions on rank 0...\n",
            "100% 250/250 [00:00<00:00, 335.07it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_causal_judgement on rank 0...\n",
            "100% 187/187 [00:00<00:00, 340.18it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_date_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.25it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_disambiguation_qa on rank 0...\n",
            "100% 250/250 [00:00<00:00, 342.14it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_formal_fallacies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 340.38it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_geometric_shapes on rank 0...\n",
            "100% 250/250 [00:00<00:00, 340.34it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_hyperbaton on rank 0...\n",
            "100% 250/250 [00:00<00:00, 344.38it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.84it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.71it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_logical_deduction_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.32it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_movie_recommendation on rank 0...\n",
            "100% 250/250 [00:00<00:00, 343.30it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_navigate on rank 0...\n",
            "100% 250/250 [00:00<00:00, 340.15it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_object_counting on rank 0...\n",
            "100% 250/250 [00:00<00:00, 332.39it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_penguins_in_a_table on rank 0...\n",
            "100% 146/146 [00:00<00:00, 340.14it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_reasoning_about_colored_objects on rank 0...\n",
            "100% 250/250 [00:01<00:00, 197.72it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_ruin_names on rank 0...\n",
            "100% 250/250 [00:00<00:00, 339.63it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_salient_translation_error_detection on rank 0...\n",
            "100% 250/250 [00:00<00:00, 334.47it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_snarks on rank 0...\n",
            "100% 178/178 [00:00<00:00, 341.99it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_sports_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 345.21it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_temporal_sequences on rank 0...\n",
            "100% 250/250 [00:00<00:00, 347.90it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 345.48it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 343.74it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_tracking_shuffled_objects_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 343.58it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_bbh_web_of_lies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 346.68it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_diamond on rank 0...\n",
            "100% 198/198 [00:00<00:00, 1266.23it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_extended on rank 0...\n",
            "100% 546/546 [00:00<00:00, 1269.64it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_gpqa_main on rank 0...\n",
            "100% 448/448 [00:00<00:00, 1262.01it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_algebra_hard on rank 0...\n",
            "100% 307/307 [00:00<00:00, 368.88it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_counting_and_prob_hard on rank 0...\n",
            "100% 123/123 [00:00<00:00, 368.55it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_geometry_hard on rank 0...\n",
            "100% 132/132 [00:00<00:00, 368.39it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_intermediate_algebra_hard on rank 0...\n",
            "100% 280/280 [00:00<00:00, 367.85it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_num_theory_hard on rank 0...\n",
            "100% 154/154 [00:00<00:00, 365.33it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_prealgebra_hard on rank 0...\n",
            "100% 193/193 [00:00<00:00, 366.71it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_math_precalculus_hard on rank 0...\n",
            "100% 135/135 [00:00<00:00, 369.34it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_ifeval on rank 0...\n",
            "100% 541/541 [00:00<00:00, 96230.64it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_murder_mysteries on rank 0...\n",
            "100% 250/250 [00:00<00:00, 2391.93it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_object_placements on rank 0...\n",
            "100% 256/256 [00:00<00:00, 2356.83it/s]\n",
            "INFO:lm_eval.api.task:Building contexts for leaderboard_musr_team_allocation on rank 0...\n",
            "100% 250/250 [00:00<00:00, 2404.51it/s]\n",
            "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
            "Running loglikelihood requests:   0% 0/152668 [00:00<?, ?it/s]ERROR 05-15 16:39:30 [core.py:398] EngineCore encountered a fatal error.\n",
            "ERROR 05-15 16:39:30 [core.py:398] Traceback (most recent call last):\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\n",
            "ERROR 05-15 16:39:30 [core.py:398]     engine_core.run_busy_loop()\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\n",
            "ERROR 05-15 16:39:30 [core.py:398]     self._process_engine_step()\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\n",
            "ERROR 05-15 16:39:30 [core.py:398]     outputs = self.step_fn()\n",
            "ERROR 05-15 16:39:30 [core.py:398]               ^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 203, in step\n",
            "ERROR 05-15 16:39:30 [core.py:398]     output = self.model_executor.execute_model(scheduler_output)\n",
            "ERROR 05-15 16:39:30 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n",
            "ERROR 05-15 16:39:30 [core.py:398]     output = self.collective_rpc(\"execute_model\",\n",
            "ERROR 05-15 16:39:30 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
            "ERROR 05-15 16:39:30 [core.py:398]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
            "ERROR 05-15 16:39:30 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 2456, in run_method\n",
            "ERROR 05-15 16:39:30 [core.py:398]     return func(*args, **kwargs)\n",
            "ERROR 05-15 16:39:30 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "ERROR 05-15 16:39:30 [core.py:398]     return func(*args, **kwargs)\n",
            "ERROR 05-15 16:39:30 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 268, in execute_model\n",
            "ERROR 05-15 16:39:30 [core.py:398]     output = self.model_runner.execute_model(scheduler_output)\n",
            "ERROR 05-15 16:39:30 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "ERROR 05-15 16:39:30 [core.py:398]     return func(*args, **kwargs)\n",
            "ERROR 05-15 16:39:30 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1173, in execute_model\n",
            "ERROR 05-15 16:39:30 [core.py:398]     prompt_logprobs_dict = self._get_prompt_logprobs_dict(\n",
            "ERROR 05-15 16:39:30 [core.py:398]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1421, in _get_prompt_logprobs_dict\n",
            "ERROR 05-15 16:39:30 [core.py:398]     logprobs = self.sampler.compute_logprobs(logits)\n",
            "ERROR 05-15 16:39:30 [core.py:398]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398]   File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/sample/sampler.py\", line 134, in compute_logprobs\n",
            "ERROR 05-15 16:39:30 [core.py:398]     return logits.log_softmax(dim=-1, dtype=torch.float32)\n",
            "ERROR 05-15 16:39:30 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 05-15 16:39:30 [core.py:398] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacity of 22.16 GiB of which 2.77 GiB is free. Process 1856034 has 19.38 GiB memory in use. Of the allocated memory 18.54 GiB is allocated by PyTorch, with 57.00 MiB allocated in private pools (e.g., CUDA Graphs), and 133.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Process EngineCore_0:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/__main__.py\", line 449, in cli_evaluate\n",
            "    results = evaluator.simple_evaluate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 439, in _wrapper\n",
            "Traceback (most recent call last):\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 338, in simple_evaluate\n",
            "    results = evaluate(\n",
            "              ^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 439, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 570, in evaluate\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\n",
            "    engine_core.run_busy_loop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\n",
            "    self._process_engine_step()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\n",
            "    outputs = self.step_fn()\n",
            "              ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 203, in step\n",
            "    output = self.model_executor.execute_model(scheduler_output)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n",
            "    output = self.collective_rpc(\"execute_model\",\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
            "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 2456, in run_method\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_worker.py\", line 268, in execute_model\n",
            "    output = self.model_runner.execute_model(scheduler_output)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1173, in execute_model\n",
            "    prompt_logprobs_dict = self._get_prompt_logprobs_dict(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1421, in _get_prompt_logprobs_dict\n",
            "    logprobs = self.sampler.compute_logprobs(logits)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/sample/sampler.py\", line 134, in compute_logprobs\n",
            "    return logits.log_softmax(dim=-1, dtype=torch.float32)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacity of 22.16 GiB of which 2.77 GiB is free. Process 1856034 has 19.38 GiB memory in use. Of the allocated memory 18.54 GiB is allocated by PyTorch, with 57.00 MiB allocated in private pools (e.g., CUDA Graphs), and 133.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/api/model.py\", line 382, in loglikelihood\n",
            "    return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/vllm_causallms.py\", line 501, in _loglikelihood_tokens\n",
            "    outputs = self._model_generate(requests=inputs, generate=False)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/vllm_causallms.py\", line 295, in _model_generate\n",
            "    outputs = self.model.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 1196, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 473, in generate\n",
            "    outputs = self._run_engine(use_tqdm=use_tqdm)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 1423, in _run_engine\n",
            "    step_outputs = self.llm_engine.step()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/llm_engine.py\", line 218, in step\n",
            "    outputs = self.engine_core.get_output()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core_client.py\", line 558, in get_output\n",
            "    raise self._format_exception(outputs) from None\n",
            "vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n",
            "Exception ignored in atexit callback: <bound method finalize._exitfunc of <class 'weakref.finalize'>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/weakref.py\", line 666, in _exitfunc\n",
            "    f()\n",
            "  File \"/usr/lib/python3.11/weakref.py\", line 590, in __call__\n",
            "    return info.func(*info.args, **(info.kwargs or {}))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tempfile.py\", line 1072, in _cleanup\n",
            "    cls._rmtree(name, ignore_errors=ignore_errors)\n",
            "  File \"/usr/lib/python3.11/tempfile.py\", line 1068, in _rmtree\n",
            "    _rmtree(name, onerror=onerror)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 752, in rmtree\n",
            "    _rmtree_safe_fd(fd, path, onerror)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 643, in _rmtree_safe_fd\n",
            "    with os.scandir(topfd) as scandir_it:\n",
            "         ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/v1/engine/core.py\", line 371, in signal_handler\n",
            "    raise SystemExit()\n",
            "SystemExit: \n",
            "[rank0]:[W515 16:39:31.634013403 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "Running loglikelihood requests:   0% 0/152668 [00:14<?, ?it/s]\n"
          ]
        }
      ]
    }
  ]
}